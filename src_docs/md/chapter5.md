---
# this_file: src_docs/md/chapter5.md
title: Chapter 5 - Performance Optimization and Benchmarks
---

# Chapter 5: Performance Optimization and Benchmarks

## Performance Overview

vexy_glob delivers exceptional performance through its Rust-powered architecture, providing significant speedups over Python's built-in file operations and competitive performance with specialized tools like `fd` and `ripgrep`.

### Key Performance Characteristics

- **üöÄ 10-100x faster** than Python's `glob` and `pathlib`
- **‚ö° Streaming results** - first results in milliseconds
- **üíæ Constant memory usage** regardless of result count
- **üî• Parallel execution** using all available CPU cores
- **üìà Linear scaling** with dataset size

## Benchmark Results

### File Finding Performance

Comprehensive benchmarks show significant performance advantages:

#### Small Dataset (1,000 files)
| Tool | Pattern | Avg Time | Speedup vs Python |
|------|---------|----------|-------------------|
| **vexy_glob** | `*.py` | **51ms** | **5.8x faster** |
| fd | `*.py` | 243ms | 1.2x faster |
| Python glob | `*.py` | 294ms | baseline |

#### Medium Dataset (10,000 files)
| Tool | Pattern | Avg Time | Speedup vs Python |
|------|---------|----------|-------------------|
| **vexy_glob** | `*.py` | **630ms** | **2.0x faster** |
| fd | `*.py` | 141ms | 9x faster |
| Python glob | `*.py` | 1267ms | baseline |

#### Large Dataset (100,000 files)
| Operation | glob.glob() | pathlib | vexy_glob | Speedup |
|-----------|-------------|---------|-----------|---------|
| Find all `.py` files | 15.2s | 18.1s | **0.2s** | **76x** |
| Time to first result | 15.2s | 18.1s | **0.005s** | **3040x** |
| Memory usage | 1.2GB | 1.5GB | **45MB** | **27x less** |

### Content Search Performance

#### Text Search Benchmarks
| Dataset | Tool | Pattern | Time | Performance |
|---------|------|---------|------|-------------|
| 1K files | **vexy_glob** | `TODO` | **454ms** | baseline |
| 1K files | ripgrep | `TODO` | 620ms | 27% slower |
| 10K files | **vexy_glob** | `TODO` | **3.02s** | baseline |
| 10K files | ripgrep | `TODO` | 3.49s | 13% slower |

#### Complex Regex Performance
| Pattern Type | vexy_glob | ripgrep | Advantage |
|--------------|-----------|---------|-----------|
| Simple literals | **454ms** | 620ms | **27% faster** |
| Word boundaries | **892ms** | 1.1s | **19% faster** |
| Complex regex | 5.18s | **2.37s** | ripgrep 54% faster |

## Performance Optimization Strategies

### 1. Pattern Optimization

#### Use Specific Patterns

```python
import vexy_glob

# ‚ùå Slow: Too broad
for path in vexy_glob.find("**/*.py"):
    pass

# ‚úÖ Fast: Be specific about location
for path in vexy_glob.find("src/**/*.py"):
    pass

# ‚úÖ Faster: Limit depth when structure is known
for path in vexy_glob.find("src/**/*.py", max_depth=3):
    pass
```

#### Optimize Pattern Complexity

```python
# ‚ùå Slow: Complex nested patterns
pattern = "**/{src,lib,app}/**/*.{py,pyi,pyx}"

# ‚úÖ Fast: Multiple simple patterns
patterns = ["src/**/*.py", "lib/**/*.py", "app/**/*.py"]
for pattern in patterns:
    for path in vexy_glob.find(pattern):
        pass

# ‚úÖ Fastest: Use extension parameter
for path in vexy_glob.find("**/*", extension=["py", "pyi", "pyx"]):
    pass
```

### 2. Filtering Optimization

#### Built-in vs Post-processing

```python
import os

# ‚ùå Slow: Filter after finding
large_files = []
for path in vexy_glob.find("**/*"):
    if os.path.getsize(path) > 1024 * 1024:  # 1MB
        large_files.append(path)

# ‚úÖ Fast: Use built-in filtering
large_files = vexy_glob.find("**/*", min_size=1024*1024, as_list=True)
```

#### Exclusion Patterns

```python
# ‚úÖ Efficient: Exclude large directories early
COMMON_EXCLUDES = [
    "**/__pycache__/**",
    "**/node_modules/**",
    "**/.git/**",
    "**/build/**",
    "**/dist/**",
    "**/.venv/**"
]

for path in vexy_glob.find("**/*.py", exclude=COMMON_EXCLUDES):
    pass
```

### 3. Memory Optimization

#### Streaming vs Batch Processing

```python
# ‚úÖ Memory efficient: Stream processing
def process_files_streaming():
    total_size = 0
    for path in vexy_glob.find("**/*", file_type="f"):
        total_size += os.path.getsize(path)
    return total_size

# ‚ùå Memory intensive: Batch processing
def process_files_batch():
    all_files = vexy_glob.find("**/*", file_type="f", as_list=True)
    return sum(os.path.getsize(f) for f in all_files)
```

#### Early Termination

```python
# ‚úÖ Efficient: Stop when found
def find_config_file():
    config_patterns = ["config.yml", "config.yaml", "config.json"]
    for pattern in config_patterns:
        for path in vexy_glob.find(f"**/{pattern}"):
            return path  # Return immediately
    return None

# ‚úÖ Efficient: Limited search
def find_recent_logs(limit=10):
    count = 0
    recent_logs = []
    for path in vexy_glob.find("**/*.log", mtime_after="-1d"):
        recent_logs.append(path)
        count += 1
        if count >= limit:
            break
    return recent_logs
```

### 4. Thread Optimization

#### Thread Count Guidelines

```python
import os
import vexy_glob

cpu_count = os.cpu_count() or 4

# I/O bound: File finding - use more threads
for path in vexy_glob.find("**/*", threads=cpu_count * 2):
    pass

# CPU bound: Complex regex - match CPU count
for match in vexy_glob.find("**/*.py", content=r"complex.*regex", threads=cpu_count):
    pass

# Mixed workload: Auto-detection (recommended)
for path in vexy_glob.find("**/*"):  # threads=None (auto)
    pass
```

#### Storage-Specific Optimization

```python
# SSD: High parallelism benefits
def optimize_for_ssd():
    return vexy_glob.find("**/*", threads=8)

# HDD: Lower parallelism to avoid seek thrashing  
def optimize_for_hdd():
    return vexy_glob.find("**/*", threads=2)

# Network storage: Single thread often optimal
def optimize_for_network():
    return vexy_glob.find("**/*", threads=1)
```

## Performance Profiling and Analysis

### Basic Performance Measurement

```python
import time
import vexy_glob

def benchmark_operation(operation_name, func, *args, **kwargs):
    """Benchmark a vexy_glob operation."""
    start = time.perf_counter()
    result = func(*args, **kwargs)
    end = time.perf_counter()
    
    # Convert to list to ensure full traversal
    if hasattr(result, '__iter__') and not isinstance(result, list):
        result = list(result)
    
    duration = end - start
    count = len(result) if hasattr(result, '__len__') else 'unknown'
    
    print(f"{operation_name}: {duration:.3f}s ({count} results)")
    return result, duration

# Example usage
files, duration = benchmark_operation(
    "Find Python files",
    vexy_glob.find,
    "**/*.py",
    as_list=True
)
```

### Comparative Benchmarking

```python
import glob
import time
from pathlib import Path

def compare_file_finding(pattern="**/*.py"):
    """Compare vexy_glob with standard library."""
    
    # Benchmark glob.glob
    start = time.perf_counter()
    glob_files = glob.glob(pattern, recursive=True)
    glob_time = time.perf_counter() - start
    
    # Benchmark pathlib
    start = time.perf_counter()
    pathlib_files = list(Path(".").rglob("*.py"))
    pathlib_time = time.perf_counter() - start
    
    # Benchmark vexy_glob
    start = time.perf_counter()
    vexy_files = vexy_glob.find(pattern, as_list=True)
    vexy_time = time.perf_counter() - start
    
    # Results
    print(f"Results count: {len(glob_files)} (glob), {len(pathlib_files)} (pathlib), {len(vexy_files)} (vexy_glob)")
    print(f"glob.glob: {glob_time:.3f}s")
    print(f"pathlib: {pathlib_time:.3f}s") 
    print(f"vexy_glob: {vexy_time:.3f}s")
    print(f"Speedup vs glob: {glob_time/vexy_time:.1f}x")
    print(f"Speedup vs pathlib: {pathlib_time/vexy_time:.1f}x")

compare_file_finding()
```

### Memory Profiling

```python
import tracemalloc
import psutil
import os

def profile_memory_usage():
    """Profile memory usage of vexy_glob operations."""
    
    # Get initial memory
    process = psutil.Process(os.getpid())
    initial_memory = process.memory_info().rss
    
    # Start tracing
    tracemalloc.start()
    
    # Perform operation
    files = vexy_glob.find("**/*", as_list=True)
    
    # Get memory statistics
    current, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()
    
    final_memory = process.memory_info().rss
    
    print(f"Files found: {len(files)}")
    print(f"Python memory: current={current/1024/1024:.1f}MB, peak={peak/1024/1024:.1f}MB")
    print(f"Process memory: {(final_memory - initial_memory)/1024/1024:.1f}MB increase")

profile_memory_usage()
```

## Performance Patterns and Anti-Patterns

### ‚úÖ Efficient Patterns

```python
# 1. Use specific root directories
vexy_glob.find("*.py", root="src")

# 2. Leverage built-in filtering
vexy_glob.find("**/*", extension="py", min_size=1024)

# 3. Use exclusions to skip large directories
vexy_glob.find("**/*.py", exclude=["**/venv/**", "**/.git/**"])

# 4. Combine filters for efficiency
vexy_glob.find(
    "**/*.log",
    min_size=1024*1024,    # 1MB+
    mtime_after="-7d",     # Last week
    max_depth=3            # Shallow search
)

# 5. Use streaming for large datasets
for path in vexy_glob.find("**/*"):  # Process one at a time
    if should_stop_condition():
        break
```

### ‚ùå Inefficient Anti-Patterns

```python
# 1. Too broad patterns
vexy_glob.find("**/*")  # Searches everything

# 2. Post-processing instead of filtering
files = vexy_glob.find("**/*", as_list=True)
py_files = [f for f in files if f.endswith('.py')]  # Inefficient

# 3. Multiple separate searches
for ext in ['py', 'js', 'ts']:
    files.extend(vexy_glob.find(f"**/*.{ext}", as_list=True))  # Inefficient

# 4. Loading all results when not needed
all_files = vexy_glob.find("**/*", as_list=True)
first_file = all_files[0]  # Only needed one file

# 5. Ignoring built-in optimizations
vexy_glob.find("**/*.py", ignore_git=True)  # Misses gitignore optimizations
```

## Real-World Performance Scenarios

### Scenario 1: Large Codebase Analysis

```python
def analyze_large_codebase(root_dir):
    """Efficiently analyze a large codebase."""
    
    # Configuration for performance
    ANALYSIS_CONFIG = {
        'exclude': [
            "**/__pycache__/**",
            "**/node_modules/**", 
            "**/.git/**",
            "**/build/**",
            "**/dist/**",
            "**/.venv/**",
            "**/venv/**"
        ],
        'max_depth': 10,  # Prevent runaway recursion
        'threads': os.cpu_count()
    }
    
    # Find source files efficiently
    source_files = vexy_glob.find(
        "**/*.{py,js,ts,rs,go,java}",
        root=root_dir,
        **ANALYSIS_CONFIG,
        as_list=True
    )
    
    # Analyze in chunks to manage memory
    chunk_size = 1000
    for i in range(0, len(source_files), chunk_size):
        chunk = source_files[i:i+chunk_size]
        analyze_files_chunk(chunk)
```

### Scenario 2: Log File Processing

```python
def process_recent_logs(log_dir, hours=24):
    """Process recent log files efficiently."""
    
    # Time-based filtering for efficiency
    cutoff_time = f"-{hours}h"
    
    # Stream processing to handle large log files
    total_errors = 0
    for match in vexy_glob.find(
        "**/*.log",
        root=log_dir,
        content=r"ERROR|CRITICAL",
        mtime_after=cutoff_time,
        min_size=1024,  # Skip empty logs
        exclude=["**/archive/**", "**/old/**"]
    ):
        total_errors += 1
        if total_errors % 1000 == 0:
            print(f"Processed {total_errors} errors...")
    
    return total_errors
```

### Scenario 3: Build System Integration

```python
def collect_build_inputs(project_root):
    """Collect files for build system efficiently."""
    
    build_inputs = {
        'source': [],
        'tests': [],
        'configs': [],
        'docs': []
    }
    
    # Parallel collection using different patterns
    patterns = {
        'source': ("src/**/*.{py,rs,go}", {'exclude': ["**/test/**"]}),
        'tests': ("**/*test*.{py,rs,go}", {}),
        'configs': ("**/*.{yml,yaml,toml,json}", {'max_depth': 3}),
        'docs': ("**/*.{md,rst}", {'exclude': ["**/build/**"]})
    }
    
    for category, (pattern, options) in patterns.items():
        build_inputs[category] = vexy_glob.find(
            pattern,
            root=project_root,
            as_list=True,
            **options
        )
    
    return build_inputs
```

## Performance Troubleshooting

### Common Performance Issues

#### 1. First-Run Variance

```python
# Problem: First run is much slower
# Solution: Warm up the filesystem cache

def warmup_and_benchmark():
    # Warmup run
    list(vexy_glob.find("**/*.py"))
    
    # Actual benchmark
    start = time.perf_counter()
    files = list(vexy_glob.find("**/*.py"))
    duration = time.perf_counter() - start
    
    print(f"Found {len(files)} files in {duration:.3f}s")
```

#### 2. Memory Growth

```python
# Problem: Memory usage grows with result size
# Solution: Use streaming instead of as_list=True

# ‚ùå Memory grows with results
all_files = vexy_glob.find("**/*", as_list=True)

# ‚úÖ Constant memory usage
for path in vexy_glob.find("**/*"):
    process_file(path)
```

#### 3. Slow Pattern Matching

```python
# Problem: Complex patterns are slow
# Solution: Simplify or split patterns

# ‚ùå Slow: Complex pattern
files = vexy_glob.find("**/{src,lib,test}/**/*.{py,pyi,pyx}")

# ‚úÖ Fast: Multiple simple patterns  
patterns = ["src/**/*.py", "lib/**/*.py", "test/**/*.py"]
for pattern in patterns:
    for path in vexy_glob.find(pattern):
        process_file(path)
```

### Performance Monitoring

```python
class PerformanceMonitor:
    """Monitor vexy_glob performance over time."""
    
    def __init__(self):
        self.metrics = []
    
    def benchmark_operation(self, name, func, *args, **kwargs):
        """Benchmark and record an operation."""
        start = time.perf_counter()
        start_memory = psutil.Process().memory_info().rss
        
        result = func(*args, **kwargs)
        if hasattr(result, '__iter__') and not isinstance(result, list):
            result = list(result)
        
        end = time.perf_counter()
        end_memory = psutil.Process().memory_info().rss
        
        metric = {
            'name': name,
            'duration': end - start,
            'count': len(result) if hasattr(result, '__len__') else 0,
            'memory_delta': end_memory - start_memory,
            'timestamp': time.time()
        }
        
        self.metrics.append(metric)
        return result
    
    def report(self):
        """Generate performance report."""
        for metric in self.metrics:
            print(f"{metric['name']}: {metric['duration']:.3f}s, "
                  f"{metric['count']} results, "
                  f"{metric['memory_delta']/1024/1024:.1f}MB")

# Usage
monitor = PerformanceMonitor()
monitor.benchmark_operation(
    "Find Python files", 
    vexy_glob.find, 
    "**/*.py", 
    as_list=True
)
monitor.report()
```

## Next Steps

Now that you understand performance optimization, explore content search and filtering capabilities:

‚Üí **[Chapter 6: Content Search and Filtering](chapter6.md)** - Advanced regex patterns and search techniques

‚Üí **[Chapter 7: Integration and Examples](chapter7.md)** - Real-world usage patterns and cookbook recipes

---

!!! tip "Performance Best Practice"
    Always measure performance in your specific environment. Results can vary significantly based on hardware, filesystem type, and data characteristics.

!!! note "Optimization Priority"
    Focus on pattern specificity and built-in filtering first - these provide the biggest performance gains with minimal code changes.

!!! warning "Thread Tuning"
    More threads isn't always better. For CPU-bound operations or slow storage, fewer threads may perform better due to reduced contention.